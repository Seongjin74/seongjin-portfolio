<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>SLDVT â€“ ìˆ˜í™” ê°ì§€ ìŒì„± ë²ˆì—­ê¸°</title>
  <link rel="preconnect" href="https://fonts.googleapis.com" />
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link
    href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;600&display=swap"
    rel="stylesheet"
  />
  <!-- ê¸°ì¡´ style.css ì°¸ì¡° -->
  <link rel="stylesheet" href="style.css" />
  <style>
    /* ì¶”ê°€ ìŠ¤íƒ€ì¼: ì´ë¯¸ì§€ ë° ì˜ìƒ í¬ê¸° ì¡°ì •, í…ìŠ¤íŠ¸ ì¢Œì¸¡ ì •ë ¬ ë“± */
    .diagram-img {
      max-width: 80%;
      display: block;
      margin-bottom: 1rem;
    }
    .video-embed {
      max-width: 600px;
      width: 100%;
      height: 337px;
      margin-bottom: 2rem;
    }
    /* ë³¸ë¬¸ ë‚´ìš©ì€ ì¢Œì¸¡ ì •ë ¬ */
    .section.about p,
    .section.about ul {
      text-align: left;
      margin-bottom: 1rem;
    }
    .section.about ul {
      list-style: none;
      padding-left: 0;
    }
  </style>
</head>
<body>
  <header class="site-header">
    <div class="container header-container">
      <div class="logo">
        <a href="https://seongjin74.github.io/seongjin-portfolio/">Seongjin's Portfolio</a>
      </div>
      <nav class="site-nav">
        <ul>
          <li><a href="#about" id="navAbout">ì†Œê°œ</a></li>
          <li><a href="#content" id="navContent">í”„ë¡œì íŠ¸ ìƒì„¸</a></li>
        </ul>
      </nav>
      <div class="header-controls">
        <button id="darkModeToggle" title="ë‹¤í¬ëª¨ë“œ ì „í™˜">ğŸŒ™</button>
        <button id="langToggle" class="lang-btn" title="ì–¸ì–´ ì „í™˜">ENG</button>
      </div>
    </div>
  </header>

  <main>
    <!-- í”„ë¡œì íŠ¸ ì†Œê°œ ì„¹ì…˜ -->
    <section id="about" class="section about">
      <div class="container">
        <h1 id="aboutTitle">í”„ë¡œì íŠ¸: SLDVT â€“ ìˆ˜í™” ê°ì§€ ìŒì„± ë²ˆì—­ê¸°</h1>
        <div id="aboutContent">
          <p>
            ë³¸ í”„ë¡œì íŠ¸ëŠ” ê¹Šì´ ìˆëŠ” í•™ìŠµ ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ LSTM(Long Short-Term Memory) ì‹ ê²½ë§ì„ í™œìš©í•˜ì—¬, ìˆ˜í™” ì œìŠ¤ì²˜ë¥¼ ì‹¤ì‹œê°„ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì‹œìŠ¤í…œì„ ê°œë°œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì²­ê° ì¥ì• ì¸ê³¼ ë¹„ì¥ì• ì¸ ì‚¬ì´ì˜ ì†Œí†µ ê²©ì°¨ë¥¼ ì¤„ì´ê³ , ëª¨ë‘ê°€ ë³´ë‹¤ ì›í™œí•˜ê²Œ ì†Œí†µí•  ìˆ˜ ìˆë„ë¡ ë•ê³ ì í•©ë‹ˆë‹¤.
          </p>
          <p>
            ì‹œìŠ¤í…œì€ ì¹´ë©”ë¼ë‚˜ ì„¼ì„œë¥¼ í†µí•´ ì‚¬ìš©ìì˜ ìˆ˜í™” ì œìŠ¤ì²˜ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìº¡ì²˜í•©ë‹ˆë‹¤. ìº¡ì²˜ëœ ì˜ìƒì€ LSTM ê¸°ë°˜ì˜ í–‰ë™ ì¸ì‹ ëª¨ë¸ì„ í†µí•´ ë¶„ì„ë˜ë©°, ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì •ì  ë° ë™ì  ì œìŠ¤ì²˜ë¥¼ í•™ìŠµí•˜ì—¬ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. LSTM ë„¤íŠ¸ì›Œí¬ëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ ì‹œê°„ì  ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ìˆ˜í™”ì˜ ë™ì ì¸ íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•©ë‹ˆë‹¤.
          </p>
          <p>
            ì¸ì‹ëœ ì œìŠ¤ì²˜ëŠ” í…ìŠ¤íŠ¸ ì •ë³´ë¡œ ë³€í™˜ëœ í›„, í…ìŠ¤íŠ¸-íˆ¬-ìŠ¤í”¼ì¹˜(TTS) ê¸°ìˆ ì„ ì´ìš©í•´ ìŒì„±ìœ¼ë¡œ í•©ì„±ë©ë‹ˆë‹¤. í•©ì„±ëœ ìŒì„±ì€ ìŠ¤í”¼ì»¤ë‚˜ í—¤ë“œí°ì„ í†µí•´ ì¶œë ¥ë˜ì–´, ìˆ˜í™”ë¥¼ ëª¨ë¥´ëŠ” ì‚¬ëŒë“¤ê³¼ë„ ì›í™œí•œ ì†Œí†µì´ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.
          </p>
          <p>
            ì‹¤ì‹œê°„ ìˆ˜í™” ì¸ì‹: ì§€ì—° ì—†ì´ ì¦‰ê°ì ìœ¼ë¡œ ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í•˜ì—¬ ì†Œí†µì´ ì›í™œí•©ë‹ˆë‹¤. ë‹¤ì¤‘ ì œìŠ¤ì²˜ ì¸ì‹: ì •ì  ì œìŠ¤ì²˜ì™€ ë™ì  ì œìŠ¤ì²˜ ëª¨ë‘ë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•ì„±ê³¼ ê²¬ê³ ì„±: ë‹¤ì–‘í•œ í™˜ê²½ê³¼ ì¡°ëª… ì¡°ê±´ì—ì„œë„ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ í™œìš©í•´ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤. ì ‘ê·¼ì„±: ìˆ˜í™” ìˆ™ë ¨ë„ì— ìƒê´€ì—†ì´ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
          </p>
          <p>
            ì´ëŸ¬í•œ ì‹œìŠ¤í…œì€ ì²­ê° ì¥ì• ì¸ ë° ë‚œì²­ì¸ë“¤ì´ ì‚¬íšŒì , ì§ì—…ì  í™˜ê²½ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†Œí†µ ìˆ˜ë‹¨ì„ ì œê³µë°›ë„ë¡ ì§€ì›í•˜ë©°, ê¹Šì´ ìˆëŠ” í•™ìŠµ ê¸°ë²•ê³¼ LSTM ë„¤íŠ¸ì›Œí¬ì˜ ì—­ëŸ‰ì„ í†µí•´ ì†Œí†µì˜ ì¥ë²½ì„ í—ˆë¬´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
          </p>
        </div>
      </div>
    </section>

    <!-- ë°ëª¨ ì˜ìƒ ì„¹ì…˜ (ì•„í‚¤í…ì²˜ ë° ì›Œí¬í”Œë¡œìš° ìœ„ì— ë°°ì¹˜) -->
    <section id="demo" class="section">
      <div class="container" style="text-align: center;">
        <h2 id="demoTitle">ë°ëª¨ ì˜ìƒ</h2>
        <iframe class="video-embed" src="https://www.youtube.com/embed/rw1ZkWsNUjY" title="ë°ëª¨ ì˜ìƒ" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </div>
    </section>

    <!-- ì•„í‚¤í…ì²˜ ë° ì›Œí¬í”Œë¡œìš° ì„¹ì…˜ -->
    <section id="content" class="section">
      <div class="container">
        <h2 id="contentTitle">ì•„í‚¤í…ì²˜ ë° ì›Œí¬í”Œë¡œìš°</h2>
        <div id="contentContent">
          <h3>High Level Diagram</h3>
          <p>ìƒìœ„ ìˆ˜ì¤€ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚¸ ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤.</p>
          <img src="HLD.png" alt="High Level Diagram" class="diagram-img" />
          <h3>Workflow</h3>
          <p>ì‹œìŠ¤í…œì˜ ë™ì‘ ê³¼ì • ë° ë°ì´í„° íë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…í•œ ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤.</p>
          <img src="Workflow.jpeg" alt="Workflow Diagram" class="diagram-img" />
        </div>
      </div>
    </section>

    <!-- ì‹œìŠ¤í…œ ì„¸ë¶€ ì„¤ëª… ì„¹ì…˜ -->
    <section id="system-details" class="section">
      <div class="container">
        <h2 id="systemDetailsTitle">ì‹œìŠ¤í…œ ì„¸ë¶€ ì„¤ëª…</h2>
        <div id="systemDetailsContent">
          <h3>LSTM ëª¨ë¸ ê°œìš”</h3>
          <p>
            LSTMì€ ì¸ê³µ ì‹ ê²½ë§ì˜ í•œ ì¢…ë¥˜ë¡œ, ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ ì›í•˜ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìˆœí™˜ ì‹ ê²½ë§(RNN)ì€ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆì—ˆìœ¼ë‚˜, LSTMì€ ì…€ ìƒíƒœ(cell state)ë¼ëŠ” êµ¬ì¡°ë¥¼ ë„ì…í•´ ì´ ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤. ì…€ ìƒíƒœëŠ” ë„¤íŠ¸ì›Œí¬ì˜ ì—¬ëŸ¬ ì‹œì ì— ê±¸ì³ ì •ë³´ë¥¼ ìœ ì§€í•˜ê³  ì „ë‹¬í•˜ì—¬, ì¥ê¸°ê°„ì˜ ë°ì´í„°ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
          </p>
          <h3>ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬</h3>
          <p><strong>ë°ì´í„° ìˆ˜ì§‘:</strong> OpenCV APIë¥¼ ì´ìš©í•´ ì¹´ë©”ë¼ë¡œ 30í”„ë ˆì„ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.</p>
          <p><strong>ë°ì´í„° ì „ì²˜ë¦¬:</strong> MediaPipeë¥¼ í™œìš©í•˜ì—¬ ì–¼êµ´, ì™¼ì†, ì˜¤ë¥¸ì†, ê·¸ë¦¬ê³  ëª¸ì˜ ì¢Œí‘œê°’(ì´ 1662ê°œ)ì„ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ NumPy ë°°ì—´ë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.</p>
          <p><strong>í•™ìŠµ:</strong> ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ 200 epochs ë™ì•ˆ í›ˆë ¨í•˜ë©°, ì „ì²´ í›ˆë ¨ ê³¼ì •ì„ 30íšŒ ë°˜ë³µí•©ë‹ˆë‹¤.</p>
        </div>
      </div>
    </section>

    <!-- ì‚¬ìš©ì ëª¨ë“œì™€ í•™ìŠµ ëª¨ë“œ ì„¹ì…˜ -->
    <section id="modes" class="section">
      <div class="container">
        <h2 id="modesTitle">ì‚¬ìš©ì ëª¨ë“œì™€ í•™ìŠµ ëª¨ë“œ</h2>
        <div id="modesContent">
          <h3>Learning ëª¨ë“œ</h3>
          <ol>
            <li><strong>ì…ë ¥:</strong> ì‚¬ìš©ìê°€ í•™ìŠµí•  ë‹¨ì–´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.</li>
            <li><strong>ë°ì´í„° ì €ì¥:</strong> ì…ë ¥ëœ ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” í´ë”ë¥¼ ìƒì„±í•´ í•™ìŠµ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.</li>
            <li><strong>ìº¡ì²˜ ë° ì „ì²˜ë¦¬:</strong> OpenCVì™€ MediaPipeë¥¼ ì´ìš©í•´ 30í”„ë ˆì„ì˜ ìˆ˜í™” ë°ì´í„°ë¥¼ ìº¡ì²˜í•˜ê³ , ì „ì²˜ë¦¬ í›„ ì €ì¥í•©ë‹ˆë‹¤.</li>
            <li><strong>í•™ìŠµ:</strong> ë°ì´í„°ë¥¼ 8:2ì˜ ë¹„ìœ¨ë¡œ í›ˆë ¨ê³¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„í• í•˜ì—¬ LSTM ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.</li>
            <li><strong>ëª¨ë¸ ì €ì¥:</strong> í•™ìŠµëœ ëª¨ë¸ì€ <code>action.keras</code> íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.</li>
          </ol>
          <h3>Motion Capture(ì‚¬ìš©ì) ëª¨ë“œ</h3>
          <ol>
            <li><strong>ì‹¤ì‹œê°„ ìº¡ì²˜:</strong> OpenCVë¥¼ í†µí•´ ì‚¬ìš©ìì˜ ìˆ˜í™” ë™ì‘ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìº¡ì²˜í•©ë‹ˆë‹¤.</li>
            <li><strong>ëœë“œë§ˆí¬ ì¶”ì¶œ:</strong> MediaPipeë¡œ ì†, ì–¼êµ´, í¬ì¦ˆ ë“±ì˜ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•˜ê³ , ì‹ ë¢°ë„ê°€ ë†’ì€ ë°ì´í„°ë§Œ ì„ ë³„í•©ë‹ˆë‹¤.</li>
            <li><strong>ë°ì´í„° ëˆ„ì :</strong> ì¼ì •í•œ ì‹œí€€ìŠ¤ ê¸¸ì´(ì˜ˆ: 30í”„ë ˆì„)ë¡œ ë°ì´í„°ë¥¼ ëˆ„ì í•©ë‹ˆë‹¤.</li>
            <li><strong>ì˜ˆì¸¡:</strong> í•™ìŠµëœ LSTM ëª¨ë¸ì— ëˆ„ì ëœ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì˜ˆì¸¡ ê²°ê³¼ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.</li>
            <li><strong>ì¶œë ¥ ì¡°ê±´:</strong> ìµœê·¼ 10ê°œì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ëª¨ë‘ ë™ì¼í•˜ë©°, í•´ë‹¹ ì˜ˆì¸¡ì˜ í™•ë¥ ì´ 95%ë¥¼ ì´ˆê³¼í•˜ê³  ì´ì „ ì˜ˆì¸¡ ë‹¨ì–´ì™€ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ ìµœì¢… ë‹¨ì–´ê°€ ìŒì„±ìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.</li>
            <li><strong>ìŒì„± ë³€í™˜:</strong> Google Text-to-Speech APIë¥¼ ì‚¬ìš©í•´ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ìŠ¤í”¼ì»¤ë¥¼ í†µí•´ ì¬ìƒí•©ë‹ˆë‹¤.</li>
            <li><strong>ì¢…ë£Œ:</strong> ì‚¬ìš©ìê°€ ì¢…ë£Œ ëª…ë ¹ì„ ë‚´ë¦¬ë©´ ì‹œìŠ¤í…œì´ ì¢…ë£Œë©ë‹ˆë‹¤.</li>
          </ol>
        </div>
      </div>
    </section>

    <!-- LSTM ì„ íƒ ì´ìœ  ì„¹ì…˜ -->
    <section id="lstm-reason" class="section">
      <div class="container">
        <h2 id="lstmReasonTitle">LSTM ì„ íƒ ì´ìœ  ë° ê³ ë ¤ì‚¬í•­</h2>
        <div id="lstmReasonContent">
          <p>
            LSTMì€ ìì—°ì–´ ì²˜ë¦¬, ìŒì„± ì¸ì‹, ì œìŠ¤ì²˜ ì¸ì‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì˜¨ ëª¨ë¸ì…ë‹ˆë‹¤. ê·¸ ì¥ì ìœ¼ë¡œëŠ” ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ëŠ¥ë ¥ê³¼ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì´ ìˆìœ¼ë‚˜, ë‹¨ì ìœ¼ë¡œëŠ” ëª¨ë¸ ë³µì¡ë„ê°€ ë†’ì•„ í•™ìŠµ ì‹œê°„ê³¼ ì—°ì‚° ìì›ì´ ë§ì´ ì†Œìš”ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ìˆìŠµë‹ˆë‹¤.
          </p>
          <p>
            í˜„ì¬ ë‹¤ë¥¸ ëª¨ë¸(ì˜ˆ: Transformer, í•©ì„±ê³± ì‹ ê²½ë§, ì–‘ë°©í–¥ LSTM)ë„ ê³ ë ¤ ì¤‘ì´ë‚˜, ìš°ì„  ê¸°ë³¸ ë°ëª¨ ë‹¨ê³„ì—ì„œëŠ” LSTMì„ ì‚¬ìš©í•´ mac í™˜ê²½ì—ì„œ dmg íŒŒì¼ë¡œ ë°°í¬í•˜ëŠ” ìˆ˜ì¤€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ í”„ë ˆì„ ìˆ˜, ì˜ˆì¸¡ ì„ê³„ê°’ ë“±ì˜ ìµœì í™” ì‘ì—…ì´ ì¶”ê°€ë¡œ í•„ìš”í•˜ë©°, TensorFlow Keras ëŒ€ì‹  PyTorchë¥¼ ì‚¬ìš©í•  ê²½ìš° ë™ì  ê³„ì‚° ê·¸ë˜í”„ì˜ ì¥ì ì„ í™œìš©í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.
          </p>
        </div>
      </div>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <p>&copy; 2025 Seongjin. All Rights Reserved.</p>
    </div>
  </footer>

  <button id="btnTop" title="ìµœìƒë‹¨ìœ¼ë¡œ ì´ë™">&#8679;</button>

  <script>
    // ë‹¤í¬ ëª¨ë“œ í† ê¸€
    const darkModeToggle = document.getElementById('darkModeToggle');
    const currentTheme = localStorage.getItem('theme');
    if (currentTheme === 'dark') {
      document.body.classList.add('dark-mode');
      darkModeToggle.textContent = 'â˜€ï¸';
    }
    darkModeToggle.addEventListener('click', () => {
      document.body.classList.toggle('dark-mode');
      if (document.body.classList.contains('dark-mode')) {
        localStorage.setItem('theme', 'dark');
        darkModeToggle.textContent = 'â˜€ï¸';
      } else {
        localStorage.setItem('theme', 'light');
        darkModeToggle.textContent = 'ğŸŒ™';
      }
    });

    // ìµœìƒë‹¨ ë²„íŠ¼
    const btnTop = document.getElementById('btnTop');
    window.addEventListener('scroll', () => {
      btnTop.style.display = window.pageYOffset > 300 ? 'block' : 'none';
    });
    btnTop.addEventListener('click', () => {
      window.scrollTo({ top: 0, behavior: 'smooth' });
    });

    // ì–¸ì–´ ì „í™˜ì„ ìœ„í•œ ë²ˆì—­ í…ìŠ¤íŠ¸
    const translations = {
      kor: {
        navAbout: "ì†Œê°œ",
        navContent: "í”„ë¡œì íŠ¸ ìƒì„¸",
        aboutTitle: "í”„ë¡œì íŠ¸: SLDVT â€“ ìˆ˜í™” ê°ì§€ ìŒì„± ë²ˆì—­ê¸°",
        aboutContent: `<p>
          ë³¸ í”„ë¡œì íŠ¸ëŠ” ê¹Šì´ ìˆëŠ” í•™ìŠµ ê¸°ë²• ì¤‘ í•˜ë‚˜ì¸ LSTM(Long Short-Term Memory) ì‹ ê²½ë§ì„ í™œìš©í•˜ì—¬, ìˆ˜í™” ì œìŠ¤ì²˜ë¥¼ ì‹¤ì‹œê°„ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ì‹œìŠ¤í…œì„ ê°œë°œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì²­ê° ì¥ì• ì¸ê³¼ ë¹„ì¥ì• ì¸ ì‚¬ì´ì˜ ì†Œí†µ ê²©ì°¨ë¥¼ ì¤„ì´ê³ , ëª¨ë‘ê°€ ë³´ë‹¤ ì›í™œí•˜ê²Œ ì†Œí†µí•  ìˆ˜ ìˆë„ë¡ ë•ê³ ì í•©ë‹ˆë‹¤.
        </p>
        <p>
          ì‹œìŠ¤í…œì€ ì¹´ë©”ë¼ë‚˜ ì„¼ì„œë¥¼ í†µí•´ ì‚¬ìš©ìì˜ ìˆ˜í™” ì œìŠ¤ì²˜ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ìº¡ì²˜í•©ë‹ˆë‹¤. ìº¡ì²˜ëœ ì˜ìƒì€ LSTM ê¸°ë°˜ì˜ í–‰ë™ ì¸ì‹ ëª¨ë¸ì„ í†µí•´ ë¶„ì„ë˜ë©°, ì´ ëª¨ë¸ì€ ë‹¤ì–‘í•œ ì •ì  ë° ë™ì  ì œìŠ¤ì²˜ë¥¼ í•™ìŠµí•˜ì—¬ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤. LSTM ë„¤íŠ¸ì›Œí¬ëŠ” ì‹œí€€ìŠ¤ ë°ì´í„°ì˜ ì‹œê°„ì  ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ë§í•˜ì—¬ ìˆ˜í™”ì˜ ë™ì ì¸ íŠ¹ì„±ì„ ì˜ ë°˜ì˜í•©ë‹ˆë‹¤.
        </p>
        <p>
          ì¸ì‹ëœ ì œìŠ¤ì²˜ëŠ” í…ìŠ¤íŠ¸ ì •ë³´ë¡œ ë³€í™˜ëœ í›„, í…ìŠ¤íŠ¸-íˆ¬-ìŠ¤í”¼ì¹˜(TTS) ê¸°ìˆ ì„ ì´ìš©í•´ ìŒì„±ìœ¼ë¡œ í•©ì„±ë©ë‹ˆë‹¤. í•©ì„±ëœ ìŒì„±ì€ ìŠ¤í”¼ì»¤ë‚˜ í—¤ë“œí°ì„ í†µí•´ ì¶œë ¥ë˜ì–´, ìˆ˜í™”ë¥¼ ëª¨ë¥´ëŠ” ì‚¬ëŒë“¤ê³¼ë„ ì›í™œí•œ ì†Œí†µì´ ê°€ëŠ¥í•˜ë„ë¡ í•©ë‹ˆë‹¤.
        </p>
        <p>
          ì‹¤ì‹œê°„ ìˆ˜í™” ì¸ì‹: ì§€ì—° ì—†ì´ ì¦‰ê°ì ìœ¼ë¡œ ì œìŠ¤ì²˜ë¥¼ ì¸ì‹í•˜ì—¬ ì†Œí†µì´ ì›í™œí•©ë‹ˆë‹¤. ë‹¤ì¤‘ ì œìŠ¤ì²˜ ì¸ì‹: ì •ì  ì œìŠ¤ì²˜ì™€ ë™ì  ì œìŠ¤ì²˜ ëª¨ë‘ë¥¼ ì¸ì‹í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤. ì •í™•ì„±ê³¼ ê²¬ê³ ì„±: ë‹¤ì–‘í•œ í™˜ê²½ê³¼ ì¡°ëª… ì¡°ê±´ì—ì„œë„ ë†’ì€ ì •í™•ë„ë¥¼ ìœ ì§€í•  ìˆ˜ ìˆë„ë¡ ë‹¤ì–‘í•œ ë°ì´í„°ì…‹ì„ í™œìš©í•´ í•™ìŠµí•˜ì˜€ìŠµë‹ˆë‹¤. ì ‘ê·¼ì„±: ìˆ˜í™” ìˆ™ë ¨ë„ì— ìƒê´€ì—†ì´ ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
        </p>
        <p>
          ì´ëŸ¬í•œ ì‹œìŠ¤í…œì€ ì²­ê° ì¥ì• ì¸ ë° ë‚œì²­ì¸ë“¤ì´ ì‚¬íšŒì , ì§ì—…ì  í™˜ê²½ì—ì„œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì†Œí†µ ìˆ˜ë‹¨ì„ ì œê³µë°›ë„ë¡ ì§€ì›í•˜ë©°, ê¹Šì´ ìˆëŠ” í•™ìŠµ ê¸°ë²•ê³¼ LSTM ë„¤íŠ¸ì›Œí¬ì˜ ì—­ëŸ‰ì„ í†µí•´ ì†Œí†µì˜ ì¥ë²½ì„ í—ˆë¬´ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
        </p>`,
        demoTitle: "ë°ëª¨ ì˜ìƒ",
        contentTitle: "ì•„í‚¤í…ì²˜ ë° ì›Œí¬í”Œë¡œìš°",
        contentContent: `<h3>High Level Diagram</h3>
        <p>ìƒìœ„ ìˆ˜ì¤€ì˜ ì‹œìŠ¤í…œ êµ¬ì¡°ë¥¼ ë‚˜íƒ€ë‚¸ ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤.</p>
        <img src="HLD.png" alt="High Level Diagram" class="diagram-img" />
        <h3>Workflow</h3>
        <p>ì‹œìŠ¤í…œì˜ ë™ì‘ ê³¼ì • ë° ë°ì´í„° íë¦„ì„ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…í•œ ì›Œí¬í”Œë¡œìš°ì…ë‹ˆë‹¤.</p>
        <img src="Workflow.jpeg" alt="Workflow Diagram" class="diagram-img" />`,
        systemDetailsTitle: "ì‹œìŠ¤í…œ ì„¸ë¶€ ì„¤ëª…",
        systemDetailsContent: `<h3>LSTM ëª¨ë¸ ê°œìš”</h3>
        <p>LSTMì€ ì¸ê³µ ì‹ ê²½ë§ì˜ í•œ ì¢…ë¥˜ë¡œ, ì‹œí€€ìŠ¤ ë°ì´í„°ë¥¼ ì…ë ¥ë°›ì•„ ì›í•˜ëŠ” ì¶œë ¥ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ê¸°ì¡´ì˜ ìˆœí™˜ ì‹ ê²½ë§(RNN)ì€ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ ê°€ì§€ê³  ìˆì—ˆìœ¼ë‚˜, LSTMì€ ì…€ ìƒíƒœ(cell state)ë¼ëŠ” êµ¬ì¡°ë¥¼ ë„ì…í•´ ì´ ë¬¸ì œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤. ì…€ ìƒíƒœëŠ” ë„¤íŠ¸ì›Œí¬ì˜ ì—¬ëŸ¬ ì‹œì ì— ê±¸ì³ ì •ë³´ë¥¼ ìœ ì§€í•˜ê³  ì „ë‹¬í•˜ì—¬, ì¥ê¸°ê°„ì˜ ë°ì´í„°ë¥¼ ê¸°ì–µí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.</p>
        <h3>ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬</h3>
        <p><strong>ë°ì´í„° ìˆ˜ì§‘:</strong> OpenCV APIë¥¼ ì´ìš©í•´ ì¹´ë©”ë¼ë¡œ 30í”„ë ˆì„ì˜ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.</p>
        <p><strong>ë°ì´í„° ì „ì²˜ë¦¬:</strong> MediaPipeë¥¼ í™œìš©í•˜ì—¬ ì–¼êµ´, ì™¼ì†, ì˜¤ë¥¸ì†, ê·¸ë¦¬ê³  ëª¸ì˜ ì¢Œí‘œê°’(ì´ 1662ê°œ)ì„ ì¶”ì¶œí•˜ê³ , ì´ë¥¼ NumPy ë°°ì—´ë¡œ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.</p>
        <p><strong>í•™ìŠµ:</strong> ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ LSTM ëª¨ë¸ì„ 200 epochs ë™ì•ˆ í›ˆë ¨í•˜ë©°, ì „ì²´ í›ˆë ¨ ê³¼ì •ì„ 30íšŒ ë°˜ë³µí•©ë‹ˆë‹¤.</p>`,
        modesTitle: "ì‚¬ìš©ì ëª¨ë“œì™€ í•™ìŠµ ëª¨ë“œ",
        modesContent: `<h3>Learning ëª¨ë“œ</h3>
        <ol>
          <li><strong>ì…ë ¥:</strong> ì‚¬ìš©ìê°€ í•™ìŠµí•  ë‹¨ì–´ë¥¼ ì…ë ¥í•©ë‹ˆë‹¤.</li>
          <li><strong>ë°ì´í„° ì €ì¥:</strong> ì…ë ¥ëœ ë‹¨ì–´ì— í•´ë‹¹í•˜ëŠ” í´ë”ë¥¼ ìƒì„±í•´ í•™ìŠµ ë°ì´í„°ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.</li>
          <li><strong>ìº¡ì²˜ ë° ì „ì²˜ë¦¬:</strong> OpenCVì™€ MediaPipeë¥¼ ì´ìš©í•´ 30í”„ë ˆì„ì˜ ìˆ˜í™” ë°ì´í„°ë¥¼ ìº¡ì²˜í•˜ê³ , ì „ì²˜ë¦¬ í›„ ì €ì¥í•©ë‹ˆë‹¤.</li>
          <li><strong>í•™ìŠµ:</strong> ë°ì´í„°ë¥¼ 8:2ì˜ ë¹„ìœ¨ë¡œ í›ˆë ¨ê³¼ ê²€ì¦ ë°ì´í„°ë¡œ ë¶„í• í•˜ì—¬ LSTM ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.</li>
          <li><strong>ëª¨ë¸ ì €ì¥:</strong> í•™ìŠµëœ ëª¨ë¸ì€ <code>action.keras</code> íŒŒì¼ì— ì €ì¥ë©ë‹ˆë‹¤.</li>
        </ol>
        <h3>Motion Capture(ì‚¬ìš©ì) ëª¨ë“œ</h3>
        <ol>
          <li><strong>ì‹¤ì‹œê°„ ìº¡ì²˜:</strong> OpenCVë¥¼ í†µí•´ ì‚¬ìš©ìì˜ ìˆ˜í™” ë™ì‘ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìº¡ì²˜í•©ë‹ˆë‹¤.</li>
          <li><strong>ëœë“œë§ˆí¬ ì¶”ì¶œ:</strong> MediaPipeë¡œ ì†, ì–¼êµ´, í¬ì¦ˆ ë“±ì˜ ëœë“œë§ˆí¬ë¥¼ ì¶”ì¶œí•˜ê³ , ì‹ ë¢°ë„ê°€ ë†’ì€ ë°ì´í„°ë§Œ ì„ ë³„í•©ë‹ˆë‹¤.</li>
          <li><strong>ë°ì´í„° ëˆ„ì :</strong> ì¼ì •í•œ ì‹œí€€ìŠ¤ ê¸¸ì´(ì˜ˆ: 30í”„ë ˆì„)ë¡œ ë°ì´í„°ë¥¼ ëˆ„ì í•©ë‹ˆë‹¤.</li>
          <li><strong>ì˜ˆì¸¡:</strong> í•™ìŠµëœ LSTM ëª¨ë¸ì— ëˆ„ì ëœ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ì—¬ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì˜ˆì¸¡ ê²°ê³¼ ì¤‘ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ë‹¨ì–´ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.</li>
          <li><strong>ì¶œë ¥ ì¡°ê±´:</strong> ìµœê·¼ 10ê°œì˜ ì˜ˆì¸¡ ê²°ê³¼ê°€ ëª¨ë‘ ë™ì¼í•˜ë©°, í•´ë‹¹ ì˜ˆì¸¡ì˜ í™•ë¥ ì´ 95%ë¥¼ ì´ˆê³¼í•˜ê³  ì´ì „ ì˜ˆì¸¡ ë‹¨ì–´ì™€ ë‹¤ë¥¼ ê²½ìš°ì—ë§Œ ìµœì¢… ë‹¨ì–´ê°€ ìŒì„±ìœ¼ë¡œ ì¶œë ¥ë©ë‹ˆë‹¤.</li>
          <li><strong>ìŒì„± ë³€í™˜:</strong> Google Text-to-Speech APIë¥¼ ì‚¬ìš©í•´ ì˜ˆì¸¡ ë‹¨ì–´ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³ , ìŠ¤í”¼ì»¤ë¥¼ í†µí•´ ì¬ìƒí•©ë‹ˆë‹¤.</li>
          <li><strong>ì¢…ë£Œ:</strong> ì‚¬ìš©ìê°€ ì¢…ë£Œ ëª…ë ¹ì„ ë‚´ë¦¬ë©´ ì‹œìŠ¤í…œì´ ì¢…ë£Œë©ë‹ˆë‹¤.</li>
        </ol>`,
        lstmReasonTitle: "LSTM ì„ íƒ ì´ìœ  ë° ê³ ë ¤ì‚¬í•­",
        lstmReasonContent: `<p>LSTMì€ ìì—°ì–´ ì²˜ë¦¬, ìŒì„± ì¸ì‹, ì œìŠ¤ì²˜ ì¸ì‹ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì˜¨ ëª¨ë¸ì…ë‹ˆë‹¤. ê·¸ ì¥ì ìœ¼ë¡œëŠ” ì‹œí€€ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ëŠ¥ë ¥ê³¼ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ í•´ê²° ëŠ¥ë ¥ì´ ìˆìœ¼ë‚˜, ë‹¨ì ìœ¼ë¡œëŠ” ëª¨ë¸ ë³µì¡ë„ê°€ ë†’ì•„ í•™ìŠµ ì‹œê°„ê³¼ ì—°ì‚° ìì›ì´ ë§ì´ ì†Œìš”ë  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ìˆìŠµë‹ˆë‹¤.</p>
        <p>í˜„ì¬ ë‹¤ë¥¸ ëª¨ë¸(ì˜ˆ: Transformer, í•©ì„±ê³± ì‹ ê²½ë§, ì–‘ë°©í–¥ LSTM)ë„ ê³ ë ¤ ì¤‘ì´ë‚˜, ìš°ì„  ê¸°ë³¸ ë°ëª¨ ë‹¨ê³„ì—ì„œëŠ” LSTMì„ ì‚¬ìš©í•´ mac í™˜ê²½ì—ì„œ dmg íŒŒì¼ë¡œ ë°°í¬í•˜ëŠ” ìˆ˜ì¤€ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì •í™•ë„ í–¥ìƒì„ ìœ„í•´ í”„ë ˆì„ ìˆ˜, ì˜ˆì¸¡ ì„ê³„ê°’ ë“±ì˜ ìµœì í™” ì‘ì—…ì´ ì¶”ê°€ë¡œ í•„ìš”í•˜ë©°, TensorFlow Keras ëŒ€ì‹  PyTorchë¥¼ ì‚¬ìš©í•  ê²½ìš° ë™ì  ê³„ì‚° ê·¸ë˜í”„ì˜ ì¥ì ì„ í™œìš©í•  ìˆ˜ ìˆì„ ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.</p>`
      },
      eng: {
        navAbout: "About",
        navContent: "Project Details",
        aboutTitle: "Project: SLDVT â€“ Sign Language Detection Voice Translator",
        aboutContent: `<p>
          This project aims to develop a system that converts sign language gestures into real-time speech by utilizing LSTM (Long Short-Term Memory) neural networksâ€”one of the advanced deep learning techniques. It is designed to reduce the communication gap between the deaf and the hearing and facilitate smoother communication for everyone.
        </p>
        <p>
          The system captures the user's sign language gestures in real time using a camera or sensor. The captured video is analyzed by an LSTM-based action recognition model that is trained to recognize various static and dynamic gestures. The LSTM network effectively models the temporal dependencies of sequence data to reflect the dynamic characteristics of sign language.
        </p>
        <p>
          The recognized gestures are converted into text, which is then synthesized into speech using text-to-speech (TTS) technology. The synthesized speech is output through speakers or headphones, enabling smooth communication with those who do not understand sign language.
        </p>
        <p>
          Real-time sign language recognition: Gestures are recognized instantly without delay, ensuring smooth communication. Multi-gesture recognition: The model is trained to recognize both static and dynamic gestures. Accuracy and robustness: The model is trained on diverse datasets to maintain high accuracy even in varying environments and lighting conditions. Accessibility: The system is designed to be user-friendly regardless of the user's proficiency in sign language.
        </p>
        <p>
          This system supports reliable communication for the deaf and hard-of-hearing in social and professional settings by leveraging advanced learning techniques and the capabilities of LSTM networks to break down communication barriers.
        </p>`,
        demoTitle: "Demo Video",
        contentTitle: "Architecture and Workflow",
        contentContent: `<h3>High Level Diagram</h3>
        <p>This diagram represents the high-level system structure.</p>
        <img src="HLD.png" alt="High Level Diagram" class="diagram-img" />
        <h3>Workflow</h3>
        <p>This workflow visually explains the process and data flow of the system.</p>
        <img src="Workflow.jpeg" alt="Workflow Diagram" class="diagram-img" />`,
        systemDetailsTitle: "System Details",
        systemDetailsContent: `<h3>Overview of LSTM Model</h3>
        <p>LSTM is a type of artificial neural network that receives sequence data and produces desired outputs. While traditional recurrent neural networks (RNNs) suffer from long-term dependency issues, LSTM overcomes these problems by introducing a structure called the cell state, which retains and transfers information across different time steps, enabling long-term data retention.</p>
        <h3>Data Collection and Preprocessing</h3>
        <p><strong>Data Collection:</strong> Data is captured at 30 frames using the OpenCV API and a camera.</p>
        <p><strong>Data Preprocessing:</strong> MediaPipe is used to extract the coordinates of the face, left hand, right hand, and body (a total of 1662 values), which are then preprocessed into a NumPy array.</p>
        <p><strong>Training:</strong> Based on the preprocessed data, the LSTM model is trained for 200 epochs, and the entire training process is repeated 30 times.</p>`,
        modesTitle: "User Mode and Learning Mode",
        modesContent: `<h3>Learning Mode</h3>
        <ol>
          <li><strong>Input:</strong> The user enters the word to be learned.</li>
          <li><strong>Data Storage:</strong> A folder corresponding to the entered word is created to store the learning data.</li>
          <li><strong>Capture and Preprocessing:</strong> 30 frames of sign language data are captured using OpenCV and MediaPipe, preprocessed, and stored.</li>
          <li><strong>Training:</strong> The data is split into training and validation sets in an 8:2 ratio and used to train the LSTM model.</li>
          <li><strong>Model Saving:</strong> The trained model is saved in the <code>action.keras</code> file.</li>
        </ol>
        <h3>Motion Capture (User Mode)</h3>
        <ol>
          <li><strong>Real-time Capture:</strong> The user's sign language movements are captured in real time using OpenCV.</li>
          <li><strong>Landmark Extraction:</strong> Landmarks for the hands, face, and pose are extracted using MediaPipe, and only high-confidence data is selected.</li>
          <li><strong>Data Accumulation:</strong> Data is accumulated in a fixed sequence length (e.g., 30 frames).</li>
          <li><strong>Prediction:</strong> The accumulated data is input into the trained LSTM model for prediction, and the word with the highest probability is selected.</li>
          <li><strong>Output Condition:</strong> The final word is output as speech only if the last 10 predictions are identical, the prediction probability exceeds 95%, and it differs from the previously predicted word.</li>
          <li><strong>Speech Conversion:</strong> The predicted word is converted to speech using the Google Text-to-Speech API and played through speakers.</li>
          <li><strong>Termination:</strong> The system terminates when the user issues an exit command.</li>
        </ol>`,
        lstmReasonTitle: "Reasons for Choosing LSTM and Considerations",
        lstmReasonContent: `<p>LSTM has demonstrated excellent performance in various fields such as natural language processing, speech recognition, and gesture recognition. Its strengths lie in its ability to handle sequence data and resolve long-term dependency issues, while its drawbacks include high model complexity and the consequent need for extensive training time and computational resources.</p>
        <p>Although other models (e.g., Transformer, Convolutional Neural Networks, Bidirectional LSTM) are also being considered, the current basic demo utilizes LSTM and has reached the stage of deployment in macOS using dmg files. However, further optimization work on frame count, prediction thresholds, etc., is required to improve accuracy, and switching from TensorFlow Keras to PyTorch may allow leveraging the advantages of dynamic computation graphs.</p>`
      }
    };

    function setLanguage(lang) {
      const t = translations[lang];
      document.getElementById('navAbout').textContent = t.navAbout;
      document.getElementById('navContent').textContent = t.navContent;
      document.getElementById('aboutTitle').textContent = t.aboutTitle;
      document.getElementById('aboutContent').innerHTML = t.aboutContent;
      document.getElementById('demoTitle').textContent = t.demoTitle;
      document.getElementById('contentTitle').textContent = t.contentTitle;
      document.getElementById('contentContent').innerHTML = t.contentContent;
      document.getElementById('systemDetailsTitle').textContent = t.systemDetailsTitle;
      document.getElementById('systemDetailsContent').innerHTML = t.systemDetailsContent;
      document.getElementById('modesTitle').textContent = t.modesTitle;
      document.getElementById('modesContent').innerHTML = t.modesContent;
      document.getElementById('lstmReasonTitle').textContent = t.lstmReasonTitle;
      document.getElementById('lstmReasonContent').innerHTML = t.lstmReasonContent;
      localStorage.setItem('lang', lang);
    }

    document.getElementById('langToggle').addEventListener('click', () => {
      const currentLang = localStorage.getItem('lang') || 'kor';
      setLanguage(currentLang === 'kor' ? 'eng' : 'kor');
    });
    const savedLang = localStorage.getItem('lang') || 'kor';
    setLanguage(savedLang);
  </script>
</body>
</html>
